{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from model.comet import Comet\n",
    "from transformers import AdamW\n",
    "import pickle\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILES = lambda data_dir: {\n",
    "    \"train\": [\n",
    "        f\"{data_dir}/sys_dialog_texts.train.npy\",\n",
    "        f\"{data_dir}/sys_target_texts.train.npy\",\n",
    "        f\"{data_dir}/sys_emotion_texts.train.npy\",\n",
    "        f\"{data_dir}/sys_situation_texts.train.npy\",\n",
    "    ],\n",
    "    \"dev\": [\n",
    "        f\"{data_dir}/sys_dialog_texts.dev.npy\",\n",
    "        f\"{data_dir}/sys_target_texts.dev.npy\",\n",
    "        f\"{data_dir}/sys_emotion_texts.dev.npy\",\n",
    "        f\"{data_dir}/sys_situation_texts.dev.npy\",\n",
    "    ],\n",
    "    \"test\": [\n",
    "        f\"{data_dir}/sys_dialog_texts.test.npy\",\n",
    "        f\"{data_dir}/sys_target_texts.test.npy\",\n",
    "        f\"{data_dir}/sys_emotion_texts.test.npy\",\n",
    "        f\"{data_dir}/sys_situation_texts.test.npy\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "word_pairs = {\n",
    "    \"it's\": \"it is\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"there're\": \"there are\",\n",
    "}\n",
    "\n",
    "emo_map = {\n",
    "    \"surprised\": 0,\n",
    "    \"excited\": 1,\n",
    "    \"annoyed\": 2,\n",
    "    \"proud\": 3,\n",
    "    \"angry\": 4,\n",
    "    \"sad\": 5,\n",
    "    \"grateful\": 6,\n",
    "    \"lonely\": 7,\n",
    "    \"impressed\": 8,\n",
    "    \"afraid\": 9,\n",
    "    \"disgusted\": 10,\n",
    "    \"confident\": 11,\n",
    "    \"terrified\": 12,\n",
    "    \"hopeful\": 13,\n",
    "    \"anxious\": 14,\n",
    "    \"disappointed\": 15,\n",
    "    \"joyful\": 16,\n",
    "    \"prepared\": 17,\n",
    "    \"guilty\": 18,\n",
    "    \"furious\": 19,\n",
    "    \"nostalgic\": 20,\n",
    "    \"jealous\": 21,\n",
    "    \"anticipating\": 22,\n",
    "    \"embarrassed\": 23,\n",
    "    \"content\": 24,\n",
    "    \"devastated\": 25,\n",
    "    \"sentimental\": 26,\n",
    "    \"caring\": 27,\n",
    "    \"trusting\": 28,\n",
    "    \"ashamed\": 29,\n",
    "    \"apprehensive\": 30,\n",
    "    \"faithful\": 31,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [\"xIntent\", \"xAttr\", \"xWant\", \"xEffect\", \"xNeed\",\"xReact\"]\n",
    "emotion_lexicon = json.load(open(\"data/NRCDict.json\"))[0]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, init_index2word):\n",
    "        self.word2index = {str(v): int(k) for k, v in init_index2word.items()}##word:index\n",
    "        self.word2count = {str(v): 1 for k, v in init_index2word.items()}##word:count\n",
    "        self.index2word = init_index2word\n",
    "        self.n_words = len(init_index2word)\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.index_word(word.strip())\n",
    "\n",
    "    def index_word(self, word): ##把没见过的词加入词典中\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_sent(sentence):  ##处理缩写\n",
    "    sentence = sentence.lower()\n",
    "    for k, v in word_pairs.items():\n",
    "        sentence = sentence.replace(k, v)\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_commonsense(comet, item, data_dict):\n",
    "    cs_list = []\n",
    "    input_event = \" \".join(item)\n",
    "    for rel in relations:\n",
    "        cs_res = comet.generate(input_event, rel)\n",
    "        cs_res = [process_sent(item) for item in cs_res]\n",
    "        cs_list.append(cs_res)\n",
    "\n",
    "    data_dict[\"utt_cs\"].append(cs_list)\n",
    "\n",
    "def encode_ctx(vocab, items, data_dict, comet):\n",
    "    for ctx in tqdm(items):\n",
    "        ctx_list = []\n",
    "        e_list = []\n",
    "        for i, c in enumerate(ctx):\n",
    "            item = process_sent(c)\n",
    "            ctx_list.append(item)\n",
    "            vocab.index_words(item)\n",
    "            ws_pos = nltk.pos_tag(item)  # pos\n",
    "            for w in ws_pos:\n",
    "                w_p = get_wordnet_pos(w[1])\n",
    "                if w[0] not in stop_words and (\n",
    "                    w_p == wordnet.ADJ or w[0] in emotion_lexicon\n",
    "                ):\n",
    "                    e_list.append(w[0])\n",
    "\n",
    "            get_commonsense(comet, item, data_dict)\n",
    "\n",
    "        data_dict[\"context\"].append(ctx_list)\n",
    "        data_dict[\"emotion_context\"].append(e_list)\n",
    "        \n",
    "def encode(vocab, files):\n",
    "\n",
    "    data_dict = {\n",
    "        \"context\": [],\n",
    "        \"target\": [],\n",
    "        \"emotion\": [],\n",
    "        \"situation\": [],\n",
    "        \"emotion_context\": [],\n",
    "        \"utt_cs\": [],\n",
    "    }\n",
    "    comet = Comet(\"data/Comet\", device)\n",
    "\n",
    "    for i, k in enumerate(data_dict.keys()):\n",
    "        items = files[i]\n",
    "        if k == \"context\":\n",
    "            encode_ctx(vocab, items, data_dict, comet)\n",
    "        elif k == \"emotion\":\n",
    "            data_dict[k] = items\n",
    "        else:\n",
    "            for item in tqdm(items):\n",
    "                item = process_sent(item)\n",
    "                data_dict[k].append(item)\n",
    "                vocab.index_words(item)\n",
    "        if i == 3:\n",
    "            break\n",
    "    assert (\n",
    "        len(data_dict[\"context\"])\n",
    "        == len(data_dict[\"target\"])\n",
    "        == len(data_dict[\"emotion\"])\n",
    "        == len(data_dict[\"situation\"])\n",
    "        == len(data_dict[\"emotion_context\"])\n",
    "        == len(data_dict[\"utt_cs\"])\n",
    "    )\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 779/40250 [16:52<14:04:47,  1.28s/it]"
     ]
    }
   ],
   "source": [
    "vocab=Lang(\n",
    "                {\n",
    "                    0: \"UNK\",\n",
    "                    1: \"PAD\",\n",
    "                    2: \"EOS\",\n",
    "                    3: \"SOS\",\n",
    "                    4: \"USR\",\n",
    "                    5: \"SYS\",\n",
    "                    6: \"CLS\",\n",
    "                }\n",
    "            )\n",
    "files = DATA_FILES(\"data/ED\")\n",
    "train_files = [np.load(f, allow_pickle=True) for f in files[\"train\"]]\n",
    "dev_files = [np.load(f, allow_pickle=True) for f in files[\"dev\"]]\n",
    "test_files = [np.load(f, allow_pickle=True) for f in files[\"test\"]]\n",
    "\n",
    "data_train = encode(vocab, train_files)\n",
    "data_dev = encode(vocab, dev_files)\n",
    "data_test = encode(vocab, test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PICKLE\n"
     ]
    }
   ],
   "source": [
    "cache_file = \"dataset_preproc1.2.p\"\n",
    "with open(cache_file, \"wb\") as f:\n",
    "    pickle.dump([data_train, data_dev, data_test, vocab], f)\n",
    "    print(\"Saved PICKLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
